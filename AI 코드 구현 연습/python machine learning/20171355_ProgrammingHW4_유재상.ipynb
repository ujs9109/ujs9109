{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1. k-nearest neighbors algorithm (KNN)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Load Dataset [1 point]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load the csv file,'final_shuffled_breast_cancer100.csv' as `df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"final_shuffled_breast_cancer100.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Split the independent variable set and the target variable set [1 point]\n",
    "- Assign `X` to the independent variable dataset\n",
    "- Assign `y` to the target variable dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.91</td>\n",
       "      <td>21.02</td>\n",
       "      <td>124.40</td>\n",
       "      <td>994.0</td>\n",
       "      <td>0.12300</td>\n",
       "      <td>0.25760</td>\n",
       "      <td>0.318900</td>\n",
       "      <td>0.119800</td>\n",
       "      <td>0.2113</td>\n",
       "      <td>0.07115</td>\n",
       "      <td>...</td>\n",
       "      <td>20.80</td>\n",
       "      <td>27.78</td>\n",
       "      <td>149.60</td>\n",
       "      <td>1304.0</td>\n",
       "      <td>0.1873</td>\n",
       "      <td>0.59170</td>\n",
       "      <td>0.90340</td>\n",
       "      <td>0.19640</td>\n",
       "      <td>0.3245</td>\n",
       "      <td>0.11980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14.34</td>\n",
       "      <td>13.47</td>\n",
       "      <td>92.51</td>\n",
       "      <td>641.2</td>\n",
       "      <td>0.09906</td>\n",
       "      <td>0.07624</td>\n",
       "      <td>0.057240</td>\n",
       "      <td>0.046030</td>\n",
       "      <td>0.2075</td>\n",
       "      <td>0.05448</td>\n",
       "      <td>...</td>\n",
       "      <td>16.77</td>\n",
       "      <td>16.90</td>\n",
       "      <td>110.40</td>\n",
       "      <td>873.2</td>\n",
       "      <td>0.1297</td>\n",
       "      <td>0.15250</td>\n",
       "      <td>0.16320</td>\n",
       "      <td>0.10870</td>\n",
       "      <td>0.3062</td>\n",
       "      <td>0.06072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20.16</td>\n",
       "      <td>19.66</td>\n",
       "      <td>131.10</td>\n",
       "      <td>1274.0</td>\n",
       "      <td>0.08020</td>\n",
       "      <td>0.08564</td>\n",
       "      <td>0.115500</td>\n",
       "      <td>0.077260</td>\n",
       "      <td>0.1928</td>\n",
       "      <td>0.05096</td>\n",
       "      <td>...</td>\n",
       "      <td>23.06</td>\n",
       "      <td>23.03</td>\n",
       "      <td>150.20</td>\n",
       "      <td>1657.0</td>\n",
       "      <td>0.1054</td>\n",
       "      <td>0.15370</td>\n",
       "      <td>0.26060</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.3055</td>\n",
       "      <td>0.05933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.08</td>\n",
       "      <td>14.71</td>\n",
       "      <td>70.21</td>\n",
       "      <td>372.7</td>\n",
       "      <td>0.10060</td>\n",
       "      <td>0.05743</td>\n",
       "      <td>0.023630</td>\n",
       "      <td>0.025830</td>\n",
       "      <td>0.1566</td>\n",
       "      <td>0.06669</td>\n",
       "      <td>...</td>\n",
       "      <td>11.35</td>\n",
       "      <td>16.82</td>\n",
       "      <td>72.01</td>\n",
       "      <td>396.5</td>\n",
       "      <td>0.1216</td>\n",
       "      <td>0.08240</td>\n",
       "      <td>0.03938</td>\n",
       "      <td>0.04306</td>\n",
       "      <td>0.1902</td>\n",
       "      <td>0.07313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12.22</td>\n",
       "      <td>20.04</td>\n",
       "      <td>79.47</td>\n",
       "      <td>453.1</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.11520</td>\n",
       "      <td>0.081750</td>\n",
       "      <td>0.021660</td>\n",
       "      <td>0.2124</td>\n",
       "      <td>0.06894</td>\n",
       "      <td>...</td>\n",
       "      <td>13.16</td>\n",
       "      <td>24.17</td>\n",
       "      <td>85.13</td>\n",
       "      <td>515.3</td>\n",
       "      <td>0.1402</td>\n",
       "      <td>0.23150</td>\n",
       "      <td>0.35350</td>\n",
       "      <td>0.08088</td>\n",
       "      <td>0.2709</td>\n",
       "      <td>0.08839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>14.87</td>\n",
       "      <td>16.67</td>\n",
       "      <td>98.64</td>\n",
       "      <td>682.5</td>\n",
       "      <td>0.11620</td>\n",
       "      <td>0.16490</td>\n",
       "      <td>0.169000</td>\n",
       "      <td>0.089230</td>\n",
       "      <td>0.2157</td>\n",
       "      <td>0.06768</td>\n",
       "      <td>...</td>\n",
       "      <td>18.81</td>\n",
       "      <td>27.37</td>\n",
       "      <td>127.10</td>\n",
       "      <td>1095.0</td>\n",
       "      <td>0.1878</td>\n",
       "      <td>0.44800</td>\n",
       "      <td>0.47040</td>\n",
       "      <td>0.20270</td>\n",
       "      <td>0.3585</td>\n",
       "      <td>0.10650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>15.08</td>\n",
       "      <td>25.74</td>\n",
       "      <td>98.00</td>\n",
       "      <td>716.6</td>\n",
       "      <td>0.10240</td>\n",
       "      <td>0.09769</td>\n",
       "      <td>0.123500</td>\n",
       "      <td>0.065530</td>\n",
       "      <td>0.1647</td>\n",
       "      <td>0.06464</td>\n",
       "      <td>...</td>\n",
       "      <td>18.51</td>\n",
       "      <td>33.22</td>\n",
       "      <td>121.20</td>\n",
       "      <td>1050.0</td>\n",
       "      <td>0.1660</td>\n",
       "      <td>0.23560</td>\n",
       "      <td>0.40290</td>\n",
       "      <td>0.15260</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.09438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>11.26</td>\n",
       "      <td>19.83</td>\n",
       "      <td>71.30</td>\n",
       "      <td>388.1</td>\n",
       "      <td>0.08511</td>\n",
       "      <td>0.04413</td>\n",
       "      <td>0.005067</td>\n",
       "      <td>0.005664</td>\n",
       "      <td>0.1637</td>\n",
       "      <td>0.06343</td>\n",
       "      <td>...</td>\n",
       "      <td>11.93</td>\n",
       "      <td>26.43</td>\n",
       "      <td>76.38</td>\n",
       "      <td>435.9</td>\n",
       "      <td>0.1108</td>\n",
       "      <td>0.07723</td>\n",
       "      <td>0.02533</td>\n",
       "      <td>0.02832</td>\n",
       "      <td>0.2557</td>\n",
       "      <td>0.07613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>19.21</td>\n",
       "      <td>18.57</td>\n",
       "      <td>125.50</td>\n",
       "      <td>1152.0</td>\n",
       "      <td>0.10530</td>\n",
       "      <td>0.12670</td>\n",
       "      <td>0.132300</td>\n",
       "      <td>0.089940</td>\n",
       "      <td>0.1917</td>\n",
       "      <td>0.05961</td>\n",
       "      <td>...</td>\n",
       "      <td>26.14</td>\n",
       "      <td>28.14</td>\n",
       "      <td>170.10</td>\n",
       "      <td>2145.0</td>\n",
       "      <td>0.1624</td>\n",
       "      <td>0.35110</td>\n",
       "      <td>0.38790</td>\n",
       "      <td>0.20910</td>\n",
       "      <td>0.3537</td>\n",
       "      <td>0.08294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>15.12</td>\n",
       "      <td>16.68</td>\n",
       "      <td>98.78</td>\n",
       "      <td>716.6</td>\n",
       "      <td>0.08876</td>\n",
       "      <td>0.09588</td>\n",
       "      <td>0.075500</td>\n",
       "      <td>0.040790</td>\n",
       "      <td>0.1594</td>\n",
       "      <td>0.05986</td>\n",
       "      <td>...</td>\n",
       "      <td>17.77</td>\n",
       "      <td>20.24</td>\n",
       "      <td>117.70</td>\n",
       "      <td>989.5</td>\n",
       "      <td>0.1491</td>\n",
       "      <td>0.33310</td>\n",
       "      <td>0.33270</td>\n",
       "      <td>0.12520</td>\n",
       "      <td>0.3415</td>\n",
       "      <td>0.09740</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0         17.91         21.02          124.40      994.0          0.12300   \n",
       "1         14.34         13.47           92.51      641.2          0.09906   \n",
       "2         20.16         19.66          131.10     1274.0          0.08020   \n",
       "3         11.08         14.71           70.21      372.7          0.10060   \n",
       "4         12.22         20.04           79.47      453.1          0.10960   \n",
       "..          ...           ...             ...        ...              ...   \n",
       "95        14.87         16.67           98.64      682.5          0.11620   \n",
       "96        15.08         25.74           98.00      716.6          0.10240   \n",
       "97        11.26         19.83           71.30      388.1          0.08511   \n",
       "98        19.21         18.57          125.50     1152.0          0.10530   \n",
       "99        15.12         16.68           98.78      716.6          0.08876   \n",
       "\n",
       "    mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0            0.25760        0.318900             0.119800         0.2113   \n",
       "1            0.07624        0.057240             0.046030         0.2075   \n",
       "2            0.08564        0.115500             0.077260         0.1928   \n",
       "3            0.05743        0.023630             0.025830         0.1566   \n",
       "4            0.11520        0.081750             0.021660         0.2124   \n",
       "..               ...             ...                  ...            ...   \n",
       "95           0.16490        0.169000             0.089230         0.2157   \n",
       "96           0.09769        0.123500             0.065530         0.1647   \n",
       "97           0.04413        0.005067             0.005664         0.1637   \n",
       "98           0.12670        0.132300             0.089940         0.1917   \n",
       "99           0.09588        0.075500             0.040790         0.1594   \n",
       "\n",
       "    mean fractal dimension  ...  worst radius  worst texture  worst perimeter  \\\n",
       "0                  0.07115  ...         20.80          27.78           149.60   \n",
       "1                  0.05448  ...         16.77          16.90           110.40   \n",
       "2                  0.05096  ...         23.06          23.03           150.20   \n",
       "3                  0.06669  ...         11.35          16.82            72.01   \n",
       "4                  0.06894  ...         13.16          24.17            85.13   \n",
       "..                     ...  ...           ...            ...              ...   \n",
       "95                 0.06768  ...         18.81          27.37           127.10   \n",
       "96                 0.06464  ...         18.51          33.22           121.20   \n",
       "97                 0.06343  ...         11.93          26.43            76.38   \n",
       "98                 0.05961  ...         26.14          28.14           170.10   \n",
       "99                 0.05986  ...         17.77          20.24           117.70   \n",
       "\n",
       "    worst area  worst smoothness  worst compactness  worst concavity  \\\n",
       "0       1304.0            0.1873            0.59170          0.90340   \n",
       "1        873.2            0.1297            0.15250          0.16320   \n",
       "2       1657.0            0.1054            0.15370          0.26060   \n",
       "3        396.5            0.1216            0.08240          0.03938   \n",
       "4        515.3            0.1402            0.23150          0.35350   \n",
       "..         ...               ...                ...              ...   \n",
       "95      1095.0            0.1878            0.44800          0.47040   \n",
       "96      1050.0            0.1660            0.23560          0.40290   \n",
       "97       435.9            0.1108            0.07723          0.02533   \n",
       "98      2145.0            0.1624            0.35110          0.38790   \n",
       "99       989.5            0.1491            0.33310          0.33270   \n",
       "\n",
       "    worst concave points  worst symmetry  worst fractal dimension  \n",
       "0                0.19640          0.3245                  0.11980  \n",
       "1                0.10870          0.3062                  0.06072  \n",
       "2                0.14250          0.3055                  0.05933  \n",
       "3                0.04306          0.1902                  0.07313  \n",
       "4                0.08088          0.2709                  0.08839  \n",
       "..                   ...             ...                      ...  \n",
       "95               0.20270          0.3585                  0.10650  \n",
       "96               0.15260          0.2654                  0.09438  \n",
       "97               0.02832          0.2557                  0.07613  \n",
       "98               0.20910          0.3537                  0.08294  \n",
       "99               0.12520          0.3415                  0.09740  \n",
       "\n",
       "[100 rows x 30 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df.iloc[:,:-1]\n",
    "y = df.iloc[:,-1]\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Split Dataset into the train & testset [1 point]\n",
    "** When you use scikit-learn method to split the train & test set : \n",
    "- the `random_state` value has to be zero.\n",
    "- the ratio of train set and test set is as follows : 80% train set / 20% test set\n",
    "- Assign the variable names as follow : `X_train`, `X_test`, `y_train`, `y_test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train's len :  80\n",
      "X_test's len  :  20\n",
      "y_train's len :  80\n",
      "y_test's len  :  20\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test,y_train, y_test  = train_test_split(X,y ,train_size  = 0.8,random_state = 0)\n",
    "\n",
    "print(\"X_train's len : \" ,len(X_train))\n",
    "print(\"X_test's len  : \" ,len(X_test))\n",
    "print(\"y_train's len : \" ,len(y_train))\n",
    "print(\"y_test's len  : \" ,len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Load  a KNN model by scikit-learn. [1 point]\n",
    "- Assign KNN as `KNN`\n",
    "- Set the `n_neighbors` as 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN = KNeighborsClassifier(n_neighbors = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Predict on your test set. [1 point]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier()"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KNN.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0.,\n",
       "       1., 0., 1.])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = KNN.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Evaluate the prediction result of your model. \n",
    "\n",
    "- Calculate the confusion matrix which consists of `TP`, `FP`, `TN`, `FN` (True Positive, False Positive, True Negative, False Negative)\n",
    "- Calculate `accuracy rate`, `sensitivity`, `specificity`\n",
    "- Fill in the blank function in order to accomplish the aforementioned  tasks. (DO NOT USE PACKAGES IN THIS TASK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1 Calculate the confusion matrix which consists of `TP`, `FP`, `TN`, `FN` (True Positive, False Positive, True Negative, False Negative) [5 points]\n",
    "- Fill in the blank function in order to accomplish the aforementioned  tasks. (DO NOT USE PACKAGES IN THIS TASK)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_confusion(y_true, y_pred):\n",
    "    \n",
    "    \"\"\"\n",
    "    Calculate the confusion matrix from the predicted target value and the actual target value.\n",
    "    \n",
    "    Inputs\n",
    "    ------\n",
    "    y_true: actual target value\n",
    "    y_pred: predicted target value\n",
    "\n",
    "    Outputs\n",
    "    -------\n",
    "    confusion matrix components: TP, FP, TN, FN\n",
    "\n",
    "    \"\"\"\n",
    "    # TODO: Compute the compusion matrix\n",
    "    \n",
    "    TP, FP, TN, FN = 0,0,0,0\n",
    "    \n",
    "    for i in range(len(y_pred)):\n",
    "        if (y_true.values[i] == y_pred[i]) & (y_true.values[i] ==0): \n",
    "            TN +=1\n",
    "        if (y_true.values[i] == y_pred[i]) & (y_true.values[i]==1): \n",
    "            TP +=1\n",
    "        if (y_true.values[i] != y_pred[i]) & (y_true.values[i] ==0): \n",
    "            FP +=1\n",
    "        if (y_true.values[i] != y_pred[i]) & (y_true.values[i] ==1): \n",
    "            FN +=1\n",
    "\n",
    "    return TP, FP, TN, FN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2 Calculate `accuracy rate` by filling in the blank of `cal_accuracy` function. [5 points]\n",
    "- Fill in the blank function in order to accomplish the aforementioned  tasks. (DO NOT USE PACKAGES IN THIS TASK) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_accuracy(y_true, y_pred):\n",
    "    \n",
    "    \"\"\"\n",
    "    Calculate the accuracy rate from the predicted target value and the actual target value.\n",
    "    \n",
    "    Inputs\n",
    "    ------\n",
    "    y_true: actual target value\n",
    "    y_pred: predicted target value\n",
    "\n",
    "    Outputs\n",
    "    -------\n",
    "    calculataed accuracy rate\n",
    "\n",
    "    \"\"\"\n",
    "    # TODO: Compute the accuracy rate\n",
    "    correct = 0\n",
    "    for i in range(len(y_pred)):\n",
    "        if y_true.values[i] == y_pred[i]:\n",
    "            correct +=1\n",
    "        \n",
    "    accuracy = correct/len(y_pred)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3 Calculate `sensitivity` by filling in the blank of `cal_sensitivity` function. [5 points]\n",
    "- Fill in the blank function in order to accomplish the aforementioned  tasks. (DO NOT USE PACKAGES IN THIS TASK)\n",
    "\n",
    "- When it comes to the `cal_sensitivity` function, we didn't specify the exact input variables, but just include all of `TP`, `FP`, `TN`, `FN`.  You have to choose two of them and use them as the input variables of the `cal_sensitivity` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_sensitivity(TP,FP,TN,FN):\n",
    "    \n",
    "    \"\"\"\n",
    "    Calculate the sensitivity using the TP, FP, TN, and FN information obtained through the confusion matrix.    \n",
    "    \n",
    "    Inputs\n",
    "    ------\n",
    "    TP,FP,TN,FN\n",
    "\n",
    "    Outputs\n",
    "    -------\n",
    "    calculated sensitivity rate\n",
    "\n",
    "    \"\"\"\n",
    "    # TODO: Compute the sensitivity rate\n",
    "    sensitivity = TP/(TP+FN)\n",
    "    \n",
    "    return sensitivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.4 Calculate `specificity` by filling in the blank of `cal_specificity` function. [5 points]\n",
    "- Fill in the blank function in order to accomplish the aforementioned  tasks. (DO NOT USE PACKAGES IN THIS TASK)\n",
    "\n",
    "- When it comes to the `cal_specificity` function, we didn't specify the exact input variables, but just include all of `TP`, `FP`, `TN`, `FN`.  You have to choose two of them and use them as the input variables of the `cal_specificity` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_specificity(TP,FP,TN,FN):\n",
    "    \n",
    "    \"\"\"\n",
    "    Calculate the sensitivity using the TP, FP, TN, and FN information obtained through the confusion matrix.    \n",
    "    \n",
    "    Inputs\n",
    "    ------\n",
    "    TP,FP,TN,FN\n",
    "\n",
    "    Outputs\n",
    "    -------\n",
    "    calculated specificity rate\n",
    "\n",
    "    \"\"\"\n",
    "    # TODO: Compute the specificity rate\n",
    "    specificity = TN/(FP+TN)\n",
    "\n",
    "    return specificity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.5 Print all of the results [1 point]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- print all of the results (confusion matrix, accuracy , sensitivity, specificity) \n",
    "- fill in the below `print` function by your own results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix (6, 2, 12, 0)\n",
      "accuracy 0.9\n",
      "sensitivity 1.0\n",
      "specificity 0.8571428571428571\n"
     ]
    }
   ],
   "source": [
    "TP, FP, TN, FN = cal_confusion(y_test, y_pred)\n",
    "print('confusion matrix', cal_confusion(y_test, y_pred))\n",
    "print('accuracy', cal_accuracy(y_test, y_pred) )\n",
    "print('sensitivity', cal_sensitivity(TP,FP,TN,FN) )\n",
    "print('specificity', cal_specificity(TP,FP,TN,FN) ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.6 Plot accuracy results as you change the K values. [4 points]\n",
    "- Plot the accuracy results from changing the number of K = (1,2,3,4,5,10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = [1,2,3,4,5,10]\n",
    "accuracy = []\n",
    "\n",
    "for k in K:\n",
    "\n",
    "    KNN = KNeighborsClassifier(n_neighbors=k)\n",
    "    KNN.fit(X_train,y_train)\n",
    "    y_pred = KNN.predict(X_test)\n",
    "    accuracy.append(cal_accuracy(y_test, y_pred))                \n",
    "                                                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.95, 1.0, 0.9, 0.9, 0.9, 0.9]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot( K,accuracy)\n",
    "plt.plot( K,accuracy, 'ro')\n",
    "plt.ylabel('accuracy rate')\n",
    "plt.xlabel(\"'K's number\")\n",
    "plt.title('Acurracy rate with changing hyperparameter (k) ')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
