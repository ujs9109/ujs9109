{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " What is the goal of learning in data mining? Please explain the following three keywords: $\\textbf{parameter, model, generalization}$. The answer does not need to contain the exact terminology,\n",
    "but it should explain the key concepts. [2.5pt] \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{1.1}$\n",
    "\n",
    "I think that the word \"data mining\" means we find the patterns through mining our data. So, the goal of learning in data mining is to find some patterns or meaningful results by making a $\\textbf{model}$. Then how we build a appropriate model? I tihnk the answer is $\\textbf{generalization}$ At each model, there exists $\\textbf{parameter}$ and it show how strongly affects certain relationship (It depdends on Supervised and Unsupervised Learning). So by training our parameters from data, we estimate the best value of each parameters, and these trained parameters should also valid when the new data comes in. This step is called generalization and we have to increase our accuracy. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarize the following concepts in 3 âˆ¼ 4 sentences each (write it in your own words). [7.5pt]\n",
    "\n",
    "\n",
    "1.  $\\textbf{Supervised Learning}$\n",
    "2.  $\\textbf{Unsupervised Learning}$\n",
    "3.  $\\textbf{Regression}$\n",
    "4.  $\\textbf{Classication}$\n",
    "5.  $\\textbf{Clustering}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. $\\textbf{Supervised Learning}$ : From the input data $X$ , we estimate the value $y$. That is, it wants to find the relationship between independent variables and dependent variables. And there are two types of $\\textbf{Supervised Learning} : Classification and Regression.\n",
    "\n",
    "\n",
    "2. $\\textbf{Unsupervised Learning}$ : There is no dependent variable $y$, and it means Unsupervised Learning don't want to predict the unknown or future value. So, its aim is just explain between independent variables. Clustering is the common data mining technique for Description Tasks.\n",
    "\n",
    "\n",
    "3. $\\textbf{Regression}$ : It is one of the method of Supervised Learning. Its aim is to predict $\\textbf{continuous response variable}$ (output, $Y$) from given predictor variables (inputs, $X$). To build a model, we usually divide into 2 sets; Training set and Testing set. Depending on the number of indepenent varibles, there exists $\\textbf{Simple (linear) regression}$ and $\\textbf{Multiple (linear) regression}$. \n",
    "\n",
    "\n",
    "4. $\\textbf{Classication}$ : It is also one of the method of Supervised Learning. Its aim is to predict $\\textbf{categorical response variable}$ (output, $Y$) from given predictor variables (inputs, $X$). Also to build a model, we usually divide into 2 sets; Training set and Testing set. Depending on the number of possible number of cases in response variables, Response varible($y$) can be  $\\textbf{binary}$ or more than 2. \n",
    "\n",
    "\n",
    "5. $\\textbf{Clustering}$ : It is also one of the method of Unsupervised Learning. Its aim is to $\\textbf{cluster}$ each groups which have similar characteristics. That is, by analyzing each set of attributes in data, it finds similarity patterns, and grouping data. It has two way to clustering : $\\textbf{\"Intra-cluster distances are minized\"}$ and $\\textbf{\"Inter-cluster distance are maximized\"}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you derive SST = SSR + SSE, you will need to use the Equation (3.1). Please prove that\n",
    "Equation (3.1) holds. [10pt]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\Sigma_{i=1}^n(y_i -\\hat{y_i})(\\hat{y_i}-\\bar{y_i}) = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We knows the equation of $\\textbf{SST, SSR, SSE}$\n",
    "\n",
    "$\\textbf{SST} :  \\Sigma_{i=1}^n(y_i -\\bar{y_i})^2 $\n",
    "\n",
    "$\\textbf{SSR} :  \\Sigma_{i=1}^n(\\hat{y_i} -\\bar{y_i})^2 $\n",
    "\n",
    "$\\textbf{SSE} :  \\Sigma_{i=1}^n(y_i -\\hat{y_i})^2 \n",
    "\\\\\n",
    "$\n",
    "\n",
    "And, if we transform the $\\textbf{SST} \\$\n",
    "\n",
    "$\\textbf{SST} :  \\Sigma_{i=1}^n(y_i -\\bar{y_i})^2 = \\Sigma_{i=1}^n(y_i - \\hat{y_i} + \\hat{y_i} - \\bar{y_i})^2 = \\Sigma_{i=1}^n((y_i - \\hat{y_i}) + (\\hat{y_i} - \\bar{y_i}))^2 = \\Sigma_{i=1}^n(y_i -\\hat{y_i})^2 + \\Sigma_{i=1}^n(\\hat{y_i} -\\bar{y_i})^2 + 2 \\Sigma_{i=1}^n((y_i - \\hat{y_i})(\\hat{y_i} - \\bar{y_i}))$  \n",
    "\n",
    "$   = \\textbf{SSR} + \\textbf{SSE} + 2 \\Sigma_{i=1}^n((y_i - \\hat{y_i})(\\hat{y_i} - \\bar{y_i})) \n",
    "\\\\\n",
    "$\n",
    "\n",
    "So, from $\\textbf{SST = SSE+SSR}$, we have to prove why $\\Sigma_{i=1}^n(y_i -\\hat{y_i})(\\hat{y_i}-\\bar{y_i}) = 0\n",
    "\\\\\n",
    "$\n",
    "\n",
    "Also, we know $\\hat{y_i}$ and $\\bar{y_i}$\n",
    "\n",
    "$\\hat{y_i} = \\hat{\\beta_0} +\\hat{\\beta_1}x_i$ and  $\\bar{y_i} = \\hat{\\beta_0} +\\hat{\\beta_1}\\bar{x} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, if we substitue $\\hat{y_i}$ and substitue $\\hat{\\beta_0}$ from equation $\\bar{y_i}$\n",
    "\n",
    "$\\Sigma_{i=1}^n(y_i -\\hat{y_i})(\\hat{y_i}-\\bar{y_i}) = 0 \\\\\n",
    "\\Rightarrow \\Sigma_{i=1}^n(y_i -(\\hat{\\beta_0} +\\hat{\\beta_1}x_i))((\\hat{\\beta_0} +\\hat{\\beta_1}x_i)-\\bar{y_i}) = 0  \\qquad (\\hat{y_i} = \\hat{\\beta_0} +\\hat{\\beta_1}x_i)\\\\\n",
    "\\Rightarrow \\Sigma_{i=1}^n(y_i -(\\bar{y_i}- \\hat{\\beta_1}\\bar{x} +\\hat{\\beta_1}x_i))((\\hat{\\beta_0} +\\hat{\\beta_1}x_i)-(\\hat{\\beta_0} +\\hat{\\beta_1}\\bar{x})) = 0 \\qquad (\\hat{\\beta_0} = \\bar{y_i}- \\hat{\\beta_1}\\bar{x} , \\, \\bar{y_i} = \\hat{\\beta_0} +\\hat{\\beta_1}\\bar{x} ) \\\\\n",
    "\\Rightarrow \\Sigma_{i=1}^n((y_i -\\bar{y_i})- \\hat{\\beta_1}(x_i-\\bar{x}))(\\hat{\\beta_1}(x_i - \\bar{x})) = 0 \\\\\n",
    "\\Rightarrow \\Sigma_{i=1}^n(\\hat{\\beta_1}(y_i -\\bar{y_i})(x_i-\\bar{x}) - \\Sigma_{i=1}^n \\hat{\\beta_1}^2(x_i-\\bar{x})^2 = 0 \\\\ \n",
    "\\Rightarrow \\hat{\\beta_1}\\Sigma_{i=1}^n((y_i -\\bar{y_i})(x_i-\\bar{x}) - \\hat{\\beta_1}^2\\Sigma_{i=1}^n (x_i-\\bar{x})^2 = 0  \\\\\n",
    "\\Rightarrow \\hat{\\beta_1}S_{xy} - \\hat{\\beta_1}^2S_{xx} = 0 \\qquad (S_{xy} = \\Sigma_{i=1}^n(y_i-\\bar{y_i})(x_i-\\bar{x}) , \\: S_{xx} = \\Sigma_{i=1}^n(x_i-\\bar{x_i})^2) \\\\\n",
    "$\n",
    "\n",
    "since $\\hat{\\beta_1} = \\frac{S_{xy}}{S_{xx}} \\\\\n",
    "\\Rightarrow \\hat{\\beta_1}S_{xy} - \\hat{\\beta_1}^2S_{xx} = \\frac{S_{xy}^2}{S_{xx}} - \\frac{S_{xy}^2}{S_{xx}} = 0$\n",
    "\n",
    "\n",
    "So, we prove the equation $ \\; \\Sigma_{i=1}^n(y_i -\\hat{y_i})(\\hat{y_i}-\\bar{y_i}) = 0$\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{4.1}$\n",
    "\n",
    "Derive the following equations, which are related to simple linear regression. [7pt]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\hat{\\beta_0} \\sim \\mathcal{N}(\\beta_0,\\,\\sigma^{2}\\frac{\\Sigma_{}x_i^2}{n\\mathcal{S_{xx}}})$$\n",
    "\n",
    "$$\\hat{\\beta_1} \\sim \\mathcal{N}(\\beta_1,\\,\\frac{\\sigma^2}{\\mathcal{S_{xx}}})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to derive $\\hat{\\beta_1}$ first.\n",
    "We know that $\\hat{\\beta_1} = \\frac{S_{xy}}{S_{xx}}$\n",
    "\n",
    "$\\textbf{Step 1)}\\quad $\n",
    "$\\hat{\\beta_1} = \\frac{S_{xy}}{S_{xx}} = \\frac{\\Sigma_{i=1}^n(x_i-\\bar{x})(y_i-\\bar{y})}{\\Sigma_{i=1}^n(x_i-\\bar{x})^2} \\\\\n",
    "\\Rightarrow \\frac{\\Sigma_{i=1}^n(x_i-\\bar{x})(y_i)}{\\Sigma_{i=1}^n(x_i-\\bar{x})^2}- \\frac{\\Sigma_{i=1}^n(x_i-\\bar{x})(\\bar{y})}{\\Sigma_{i=1}^n(x_i-\\bar{x})^2} \\Rightarrow \\frac{\\Sigma_{i=1}^n(x_i-\\bar{x})(y_i)}{\\Sigma_{i=1}^n(x_i-\\bar{x})^2} -  \\bar{y}\\frac{\\Sigma_{i=1}^n(x_i-\\bar{x})}{\\Sigma_{i=1}^n(x_i-\\bar{x})^2}) $\n",
    "\n",
    "And the second term will be gone, because \n",
    "\n",
    "1) $\\Sigma_{i=1}^n(x_i-\\bar{x}) = \\Sigma_{i=1}^n(x_i)-n\\bar{x}$ \n",
    "\n",
    "2) $n\\bar{x} = n\\Sigma_{i=1}^n(\\frac{x_i}{n}) = \\Sigma_{i=1}^n(x_i)$\n",
    "\n",
    "by 1) & 2), $\\Sigma_{i=1}^n(x_i-\\bar{x}) = 0$\n",
    "\n",
    "Therefore, $\\hat{\\beta_1} = \\frac{\\Sigma_{i=1}^n(x_i-\\bar{x})(y_i)}{\\Sigma_{i=1}^n(x_i-\\bar{x})^2}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Step 2)} \\quad $ \n",
    "$ {E}$[$\\hat{\\beta_1}$] $ = \\frac{\\Sigma_{i=1}^n(x_i-\\bar{x})({E}[{y_i}])}{\\Sigma_{i=1}^n(x_i-\\bar{x})^2} $,$ \\quad $ where $y_i$ follows $N(\\beta_0+\\beta_1x_i,\\sigma^2)$\n",
    "\n",
    "\n",
    "$\\Rightarrow \\frac{\\Sigma_{i=1}^n(x_i-\\bar{x})({E}[{y_i}])}{\\Sigma_{i=1}^n(x_i-\\bar{x})^2} = \\frac{\\Sigma_{i=1}^n(x_i-\\bar{x})}{\\Sigma_{i=1}^n(x_i-\\bar{x})^2}\\beta_0 + \\frac{\\Sigma_{i=1}^n(x_i-\\bar{x})(x_i)}{\\Sigma_{i=1}^n(x_i-\\bar{x})^2}\\beta_1$\n",
    "\n",
    "Also, the first term will be gone away because $\\Sigma_{i=1}^n(x_i-\\bar{x}) = 0$\n",
    "\n",
    "$\\Rightarrow \\frac{\\Sigma_{i=1}^n(x_i-\\bar{x})({E}[{y_i}])}{\\Sigma_{i=1}^n(x_i-\\bar{x})^2} = \\frac{\\Sigma_{i=1}^n(x_i-\\bar{x})}{\\Sigma_{i=1}^n(x_i-\\bar{x})^2}\\beta_0 + \\frac{\\Sigma_{i=1}^n(x_i-\\bar{x})(x_i)}{\\Sigma_{i=1}^n(x_i-\\bar{x})^2}\\beta_1$\n",
    "\n",
    "Also, the first term will be gone away because $\\Sigma_{i=1}^n(x_i-\\bar{x}) = 0 $\n",
    "\n",
    "$\\Rightarrow$ ${E}$[$\\hat{\\beta_1}$] $ = \\frac{\\Sigma_{i=1}^n(x_i-\\bar{x})(x_i)}{\\Sigma_{i=1}^n(x_i-\\bar{x})^2}\\beta_1$\n",
    "\n",
    "\n",
    "Also, the second term can be trasnformed by using $\\Sigma_{i=1}^n(x_i-\\bar{x}) = 0 \\Rightarrow \\bar{x}\\Sigma_{i=1}^n(x_i-\\bar{x}) = 0$\n",
    "\n",
    "$\\Rightarrow$, $\\frac{\\Sigma_{i=1}^n(x_i-\\bar{x})(x_i)}{\\Sigma_{i=1}^n(x_i-\\bar{x})^2}\\beta_1 = \\frac{\\Sigma_{i=1}^n(x_i-\\bar{x})(x_i)}{\\Sigma_{i=1}^n(x_i-\\bar{x})^2}\\beta_1 - \\frac{\\Sigma_{i=1}^n(x_i-\\bar{x})(\\bar{x})}{\\Sigma_{i=1}^n(x_i-\\bar{x})^2}\\beta_1 = \\frac{\\Sigma_{i=1}^n(x_i-\\bar{x})(x_i-\\bar{x})}{\\Sigma_{i=1}^n(x_i-\\bar{x})^2}\\beta_1 = \\frac{\\Sigma_{i=1}^n(x_i-\\bar{x})^2}{\\Sigma_{i=1}^n(x_i-\\bar{x})^2}\\beta_1 = \\beta_1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Step 3)} \\quad $ \n",
    "$ {V}$[$\\hat{\\beta_1}$] $ = V(\\frac{\\Sigma_{i=1}^n(x_i-\\bar{x})({y_i})}{\\Sigma_{i=1}^n(x_i-\\bar{x})^2} )$, $ \\quad $ where $y_i$ follows $N(\\beta_0+\\beta_1x_i,\\sigma^2)$\n",
    "\n",
    "We know that $Var(a+bx) = b^2Var(x)$ , and when we calculate $V(\\beta_1)$, $\\frac{\\Sigma_{i=1}^n(x_i-\\bar{x})}{\\Sigma_{i=1}^n(x_i-\\bar{x})^2}$ is constant like $b$\n",
    "\n",
    "\n",
    "$\\Rightarrow {V}$[$\\hat{\\beta_1}$] $ = V(\\frac{\\Sigma_{i=1}^n(x_i-\\bar{x})({y_i})}{\\Sigma_{i=1}^n(x_i-\\bar{x})^2} )  =  V(\\frac{\\Sigma_{i=1}^n(x_i-\\bar{x})}{S_{xx}})V(y_i) $\n",
    "\n",
    "and, $ V(\\frac{\\Sigma_{i=1}^2(x_i-\\bar{x})}{S_{xx}}) = \\frac{1}{S_{xx}}$ , $V(y_i) =\\sigma^2 $\n",
    "\n",
    "$\\Rightarrow V$[$\\hat{\\beta_1}$]$ =  \\frac{\\sigma^2}{S_{xx}}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Step 4)} \\quad  \n",
    "{E}$[$\\hat{\\beta_0}$] ,$ \\quad $ where $y_i$ follows $N(\\beta_0+\\beta_1x_i,\\sigma^2)$\n",
    "\n",
    "$\\Rightarrow {E}$[$\\hat{\\beta_0}$] $= {E}$[$\\bar{y}-\\hat{\\beta_1}\\bar{x}$] $= {E}$[$\\bar{y}$] $ -  {E}$[$\\hat{\\beta_1}\\bar{x}$] , $ \\qquad (\\bar{y} = \\hat{\\beta_0}+\\hat{\\beta_1\\bar{x}}$)\n",
    "\n",
    "1) $E$[$\\bar{y}$] = $\\frac{1}{n}\\Sigma_{i=1}^nE(y_i) = \\frac{1}{n}\\Sigma_{i=1}^n(\\beta_0+\\beta_1x_i)$ ,$ \\quad $ because $E(y_i) = (\\beta_0+\\beta_1x_i)$\n",
    "\n",
    "2) ${E}$[$\\hat{\\beta_1}$] $= \\beta_1 $ ,$ \\quad $ from the $\\textbf{Step 2)'s proof}$\n",
    "\n",
    "$\\Rightarrow {E}$[$\\hat{\\beta_0}$] $= {E}$[$\\bar{y}$] $ -  {E}$[$\\hat{\\beta_1}\\bar{x}$] $=  \\frac{1}{n}\\Sigma_{i=1}^n(\\beta_0+\\beta_1x_i) - \\beta_1\\bar{x}$\n",
    "\n",
    "3) $\\frac{1}{n}\\Sigma_{i=1}^n(\\beta_0) = \\beta_0$\n",
    "\n",
    "4) $\\frac{1}{n}\\Sigma_{i=1}^n(\\beta_1x_i) = \\beta_1\\bar{x}$\n",
    "\n",
    "$\\Rightarrow \\frac{1}{n}\\Sigma_{i=1}^n(\\beta_0+\\beta_1x_i) - \\beta_1\\bar{x} = \\beta_0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Step 5)} \\quad $ \n",
    "$ {V}$[$\\hat{\\beta_0}$] $= {V}[\\bar{y}-\\hat{\\beta_1}\\bar{x}$] ,$ \\quad $ where $\\hat{\\beta_0} = \\bar{y} -\\hat{\\beta_1}\\bar{x} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And also, $V$$[\\bar{y}-\\hat{\\beta_1}\\bar{x}] = $$V$$[\\bar{y}$] $+$$V$$[-\\hat{\\beta_1}\\bar{x}$] $-2Cov(\\bar{y},\\hat{\\beta_1}\\bar{x})$ ,$ \\quad $ by $V(x-y) = V(x) +V(-y) -2Cov(x,y) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) We know $V(\\bar{y}) = \\frac{\\sigma^2}{n}, V(-\\hat{\\beta_1}\\bar{x}) = \\bar{x}^2V(\\hat{\\beta_1}) = \\bar{x}^2(\\frac{\\sigma^2}{S_{xx}}) \n",
    "\\\\\n",
    "$ \n",
    "\n",
    "from the above equation $\\textbf{Step 3)}$\n",
    "\n",
    "2) $Cov(\\bar{y},\\hat{\\beta_1}\\bar{x})= \\bar{x}Cov(\\bar{y},\\hat{\\beta_1})= Cov(\\Sigma_{i=1}^n\\frac{1}{n} y_i, \\Sigma_{j=1}^n \\frac{x_j-\\bar{x}}{S_{xx}}y_j) $\n",
    "\n",
    "$\\textbf{Since}, \\bar{y} = \\frac{1}{n}\\Sigma_{i=1}^ny_i$, $\\quad \\hat{\\beta_1} =  \\Sigma_{j=1}^n \\frac{x_j-\\bar{x}}{S_{xx}}y_j$ from $\\textbf{Step 1)}$\n",
    "\n",
    "$\\Rightarrow Cov(\\bar{y},\\hat{\\beta_1}\\bar{x}) = \\Sigma_{i=1}^n\\Sigma_{j=1}^n \\frac{x_j-\\bar{x}}{nS_{xx}}Cov(y_i,y_j)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Also},$\n",
    "$$\n",
    "Cov(y_i,y_j) =\n",
    "\\begin{cases}\n",
    "\\sigma^2, & \\text{if }y_i\\text{ and }y_j \\text{ are same.} \\\\\n",
    "0, & \\text{if }y_i\\text{ and }y_j \\text{ are not same.}\n",
    "\\end{cases}\n",
    "$$\n",
    "$\\Rightarrow Cov(\\bar{y},\\hat{\\beta_1}\\bar{x}) = \\Sigma_{i=1}^n\\Sigma_{j=1}^n \\frac{x_j-\\bar{x}}{nS_{xx}}Cov(y_i,y_j) = \\Sigma_{i=1}^n \\frac{x_i-\\bar{x}}{nS_{xx}}\\sigma^2$\n",
    "\n",
    "$\\textbf{And from Step1 & Step2}\\ \\Rightarrow \\Sigma_{i=1}^n(x_i-\\bar{x}) = 0$\n",
    "\n",
    "$Cov(y_i,y_j) = \\Sigma_{i=1}^n \\frac{x_i-\\bar{x}}{nS_{xx}}\\sigma^2 = 0$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{In conclusion, from 1) & 2)}$\n",
    "\n",
    "$ {V}$[$\\hat{\\beta_0}$] $= {V}[\\bar{y}-\\hat{\\beta_1}\\bar{x}$] \n",
    "\n",
    "= $V$$[\\bar{y}-\\hat{\\beta_1}\\bar{x}]  $\n",
    "\n",
    "= $V$$[\\bar{y}$] $+$$V$$[-\\hat{\\beta_1}\\bar{x}$] $-2Cov(\\bar{y},\\hat{\\beta_1}\\bar{x}) $\n",
    "\n",
    "=$ \\frac{\\sigma^2}{n}+\\bar{x}^2(\\frac{\\sigma^2}{S_{xx}}) - 2 \\times 0 $\n",
    "\n",
    "=$ \\frac{\\sigma^2}{n}+\\bar{x}^2(\\frac{\\sigma^2}{S_{xx}}) = \\sigma^2 (\\frac{1}{n} + \\bar{x}^2(\\frac{1}{S_{xx}})) $\n",
    "\n",
    "=$ \\sigma^2 (\\frac{S_{xx}+n\\bar{x}^2}{nS_{xx}}) = \\sigma^2 (\\frac{\\Sigma_{i=1}^n(x_i-\\bar{x})^2+n\\bar{x}^2}{nS_{xx}}) $\n",
    "\n",
    "=$ \\sigma^2 (\\frac{\\Sigma_{i=1}^n(x_i^2+\\bar{x}^2 -2x_i\\bar{x})+n\\bar{x}^2}{nS_{xx}}) $\n",
    "\n",
    "=$ \\sigma^2 \\frac{(\\Sigma_{i=1}^nx_i^2)+(n{\\bar{x}^2} -2n\\bar{x}^2+n\\bar{x}^2)}{nS_{xx}}) $\n",
    "\n",
    "=$ \\sigma^2 \\frac{(\\Sigma_{i=1}^nx_i^2)}{nS_{xx}}) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{4.2}$\n",
    "\n",
    "Equation (4.1) and (4.2) both follow a normal distribution. However, why do we have to perform\n",
    "a t-test to test the signicance of parameters? [3pt]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of all, we can not know the population's variation. ($\\sigma^2$). Instead of using $\\sigma^2$, we caculate sample's variation ($s^2$) and use this instead of $\\sigma^2$. We call this procedure \"studentization\". However, calculated variation is bias ($s^2 != \\sigma^2)$ because of degree of freedom( =df). Therefore, if the number of sample is not enough $\\textbf{(n<30)}$, it can show different ouput (under estimation), when we use the normal distribution's z-test. So, in order to prevent this, we use t-test intead of z-test.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "$cf)$ if the number of sample is enough $\\textbf{(n>30)}$, it follows the normal distribution, because of the $\\textbf{central limit Theorem (CLT)}$. Then, we can use Z-test instead of t-test. (both results are quite same). However, if the sample is not large, It cannot hold the $\\textbf{CLT}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In multiple linear regression, you have seen the following Equation (5.1). What does $V_{ii}$ mean?\n",
    "Please describe in detail. [5pt]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\hat{\\beta_i} \\sim \\mathcal{N}(\\beta_i,\\,{\\sigma^2}{V_{ii}})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{In multiple linear regression},$ there exists more than 1 independent variable $X.$ \n",
    "\n",
    "So, the equation is like below\n",
    "$$ Y_i = \\beta_0 + \\beta_1x_{i1}+ \\beta_2x_{2i} + \\cdot\\cdot\\cdot + \\beta_kx_{ik} + \\epsilon  $$\n",
    "Because, it has more than 1 of $x$ variable, we can transform into matrix form\n",
    "$$ Y_i = X\\hat{\\beta} + \\epsilon$$\n",
    "\n",
    "$ \\textbf{In order to find }\\hat{\\beta,} \\text{ we use Transform and inverse}$\n",
    "\n",
    "First, we neglect the error term,\n",
    "\n",
    "$ Y_i = X\\hat{\\beta} + \\epsilon \\approx Y_i = X\\hat{\\beta} $\n",
    "\n",
    "And then, we multiply $X^T$ at both side, in order to make square matrix\n",
    "\n",
    "$ X^TY_i = (X^TX)\\beta$\n",
    "\n",
    "$\\textbf{1.}$ Then, multiply inverse matrix $(X^TX)^{-1} $, in order to find $\\beta$\n",
    "\n",
    "$\\rightarrow (X^TX)^{-1}X^TY_i = (X^TX)^{-1}(X^TX)\\beta$ $\\Rightarrow \\beta = (X^TX)^{-1}X^TY_i$\n",
    "\n",
    "$\\textbf{2.}$ We know that $Var(c{Y}) = cVar(Y)c^T$, in vector form. So,\n",
    "\n",
    "$\\rightarrow Var(\\beta) = Var((X^TX)^{-1}X^TY_i) = [(X^TX)^{-1}X^T] Var(Y) [(X^TX)^{-1}X^T]^T $\n",
    "\n",
    "$\\textbf{3.}$ Also, we assume that all of the independent variables $X$ are i.i.d (=independent and identically distributed),\n",
    "\n",
    "$$\n",
    "\\\\Var(Y) =\n",
    "\\left\\lceil\n",
    "\\begin{matrix}\n",
    "\\sigma^2 &  0 &  \\cdot\\cdot\\cdot & 0 \\\\\n",
    "0  & \\sigma^2 &  \\cdot\\cdot\\cdot & 0 \\\\\n",
    "0 & \\cdot\\cdot\\cdot & \\cdot\\cdot\\cdot & \\cdot\\cdot\\cdot \\\\\n",
    "0 & 0 & \\cdot\\cdot\\cdot & \\sigma^2 \\\\\n",
    "\\end{matrix}\n",
    "\\right\\rceil\n",
    "= \\sigma^2I_n\n",
    "$$\n",
    "\n",
    "$\\textbf{Therefore, with 2. & 3.}$\n",
    "\n",
    "$\\rightarrow Var(\\beta) = [(X^TX)^{-1}X^T] Var(Y) [(X^TX)^{-1}X^T]^T $\n",
    "\n",
    "$ = [(X^TX)^{-1}X^T] (\\sigma^2I_n)[(X^TX)^{-1}X^T]^T  = \\sigma^2(X^TX)^{-1}$\n",
    "\n",
    "### So, if we let $V_{jj}$ be the $j_{th}$ diagonal of the matrix $(X^TX)^{-1}$\n",
    "\n",
    "$$Var(\\hat{\\beta}) = \\sigma^2V_{jj}$$\n",
    "\n",
    "$$\\textbf{where,}$$ $$V_{jj} = (X^TX)^{-1}\\text{'s jth diagonal}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a set of independent and identically distributed data points {$x_1$, â€¦ , $x_N$}, $x$ follows a distribution of $\\mathcal{N}(\\mu, \\, \\sigma^{2}$). Derive the $\\textbf{Maximum Likelihood Estimation(MLE)}$ process that\n",
    "results in parameter estimate in Equation (6.1) and (6.2). Assume that both $\\mu$ and $\\sigma$ are known.\n",
    "Please show the whole process of your derivation. [20pt]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mu_{ML} = \\frac{1}{N}\\Sigma_{t=1}^N(x_t)   $$\n",
    "$$ \\sigma^2_{ML} = \\frac{1}{N}\\Sigma_{t=1}^N(x_t - \\mu_{ML})^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Maximum Likelihood Estimation}$ is kinds of method that maximizing the possibility to exist. And since every data points follows (i.i.d) and $x$ follows normal distribution with mean = $\\mu$, variance = $\\sigma^2$.\n",
    "\n",
    "So, it follows $$pr(x_i|\\mu,\\sigma^2) = \\frac{1}{(2\\pi\\sigma^2)^{\\frac{1}{2}}}\\exp^{\\frac{-(x_i-\\mu)}{2\\sigma^2}} $$\n",
    "\n",
    "So, its likelihood function for Y is:\n",
    "$$ \\Pi_{i=1}^n(pr(x_i|\\mu,\\sigma^2) =  L = \\frac{1}{(2\\pi\\sigma^2)^{\\frac{n}{2}}}\\exp^{\\frac{-\\Sigma_{i=1}^n(x_i-\\mu)^2}{2\\sigma^2}} $$\n",
    "\n",
    "And then, log likelihood  for the data is :\n",
    "$$ logL = -\\frac{n}{2}ln(2\\pi)-\\frac{n}{2}ln(\\sigma^2) -\\Sigma_{i=1}^n\\frac{(x_i-\\mu)^2}{2\\sigma^2} $$\n",
    "\n",
    "So, we have to proof by using 'partial derivative'\n",
    "$$ \\frac{\\partial log L}{\\partial \\mu} = 0, \\frac{\\partial log L}{\\partial \\sigma^2}=0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1st trial to find  $\\frac{\\partial log L}{\\partial \\mu} = 0$\n",
    "\n",
    "$$ \\frac{\\partial log L}{\\partial \\mu} = -0-0-\\frac{-2}{2\\sigma^2}\\Sigma_{i=1}^n(x_i-\\mu) = 0 $$\n",
    "$$\\therefore \\frac{1}{\\sigma^2}\\Sigma_{i=1}^n(x_i-\\mu) = 0$$\n",
    "$$ \\therefore \\Sigma_{i=1}^n(x_i-\\mu_{ML}) = 0 \\Rightarrow \\Sigma_{i=1}^n(x_i) - n\\mu_{ML} = 0 $$\n",
    "$$ \\therefore \\mu_{ML} = \\frac{1}{n}\\Sigma_{i=1}^n(x_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1st trial to find  $\\frac{\\partial log L}{\\partial \\sigma^2} = 0$\n",
    "$$ \\frac{\\partial log L}{\\partial \\sigma^2} = -0  -\\frac{n}{2}\\frac{2\\sigma}{\\sigma^2} - \\frac{-2}{2\\sigma^3}\\Sigma_{i=1}^n(x_i-\\mu)^2 =0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\therefore -\\frac{n}{\\sigma}+\\frac{1}{\\sigma^3}\\Sigma_{i=1}^n(x_i-\\mu)^2 = 0 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\therefore \\frac{n\\sigma^2 - \\Sigma_{i=1}^n(x_i-\\mu)^2}{\\sigma^3} = 0 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numerator should be zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\therefore n\\sigma_{ML}^2 - \\Sigma_{i=1}^n(x_i-\\mu)^2 = 0 \\rightarrow \\sigma_{ML}^2 -\\frac{1}{n}\\Sigma_{i=1}^n(x_i-\\mu)^2 = 0 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\therefore \\sigma_{ML}^2 = \\frac{1}{n}\\Sigma_{i=1}^n(x_i-\\mu)^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 7\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the following data $ \\textbf{X} \\in \\mathbb{R}^{4\\times2}$ and label $y\\in \\mathbb{R}^{4\\times1}$, solve the below questions. You may\n",
    "use a matrix multiplication calculator. $ \\textbf{Explicitly state}$ all the intermediate results of the\n",
    "calculation you used to \u001bnd your answer. Otherwise, no points will be given.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$\n",
    "\\\\X =\n",
    "\\left\\lceil\n",
    "\\begin{matrix}\n",
    "1 & 2 \\\\\n",
    "-2 & -2 \\\\\n",
    "0 & 1 \\\\\n",
    "3 & 1 \\\\\n",
    "\\end{matrix}\n",
    "\\right\\rceil\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\\\y =\n",
    "\\left\\lceil\n",
    "\\begin{matrix}\n",
    "0 \\\\\n",
    "1 \\\\\n",
    "2 \\\\\n",
    "3 \\\\\n",
    "\\end{matrix}\n",
    "\\right\\rceil\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{7.1}$\n",
    "\n",
    "Fit a multiple linear regression without intercept. Find the parameters of each feature $\\beta_1$, $\\beta_2$.\n",
    "Write the answer in $\\textbf{fractional form}$ ($^*$ You do not need to consider the intercept $\\beta_0$) [5pt]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No intercpet means that \n",
    "\n",
    "$$ Y_i = x_{i1}\\beta_1+ x_{i2}\\beta_2 +x_{i3}\\beta_3 + x_{i4}\\beta_4 + \\epsilon  $$\n",
    "Because, it has 4 of $x$ variables, we can transform into matrix form\n",
    "$$ Y_i = X\\hat{\\beta} + \\epsilon$$\n",
    "\n",
    "$ \\textbf{In order to find }\\hat{\\beta,} \\text{ we use Transform and inverse}$\n",
    "\n",
    "1) , we neglect the error term,\n",
    "\n",
    "$ Y_i = X\\hat{\\beta} + \\epsilon \\approx Y_i = X\\hat{\\beta} $\n",
    "\n",
    "$$\n",
    "\\\\\n",
    "\\left\\lceil\n",
    "\\begin{matrix}\n",
    "0 \\\\\n",
    "1 \\\\\n",
    "2 \\\\\n",
    "3 \\\\\n",
    "\\end{matrix}\n",
    "\\right\\rceil\n",
    "=\n",
    "\\left\\lceil\n",
    "\\begin{matrix}\n",
    "1 & 2 \\\\\n",
    "-2 & -2 \\\\\n",
    "0 & 1 \\\\\n",
    "3 & 1 \\\\\n",
    "\\end{matrix}\n",
    "\\right\\rceil\n",
    "\\hat{\\beta}\n",
    "$$\n",
    "\n",
    "2) , we multiply $X^T$ at both side, in order to make square matrix\n",
    "\n",
    "$ X^TY_i = (X^TX)\\beta$\n",
    "\n",
    "$$\n",
    "\\\\\n",
    "\\left\\lceil\n",
    "\\begin{matrix}\n",
    "1 & -2 & 0 & 3 \\\\\n",
    "2 & -2 & 1 & 1 \\\\\n",
    "\\end{matrix}\n",
    "\\right\\rceil\n",
    "\\left\\lceil\n",
    "\\begin{matrix}\n",
    "0 \\\\\n",
    "1 \\\\\n",
    "2 \\\\\n",
    "3 \\\\\n",
    "\\end{matrix}\n",
    "\\right\\rceil\n",
    "=\n",
    "\\left\\lceil\n",
    "\\begin{matrix}\n",
    "1 & -2 & 0 & 3 \\\\\n",
    "2 & -2 & 1 & 1 \\\\\n",
    "\\end{matrix}\n",
    "\\right\\rceil\n",
    "\\left\\lceil\n",
    "\\begin{matrix}\n",
    "1 & 2 \\\\\n",
    "-2 & -2 \\\\\n",
    "0 & 1 \\\\\n",
    "3 & 1 \\\\\n",
    "\\end{matrix}\n",
    "\\right\\rceil\n",
    "\\hat{\\beta}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\\\ \\Rightarrow\n",
    "\\left\\lceil\n",
    "\\begin{matrix}\n",
    "7 \\\\\n",
    "3 \\\\\n",
    "\\end{matrix}\n",
    "\\right\\rceil\n",
    "=\n",
    "\\left\\lceil\n",
    "\\begin{matrix}\n",
    "14 & 9  \\\\\n",
    "9 & 10  \\\\\n",
    "\\end{matrix}\n",
    "\\right\\rceil\n",
    "\\hat{\\beta}\n",
    "$$\n",
    "\n",
    "3) Then, multiply inverse matrix $(X^TX)^{-1} $, in order to find $\\beta$\n",
    "\n",
    "$\\rightarrow (X^TX)^{-1}X^TY_i = (X^TX)^{-1}(X^TX)\\beta$ $\\Rightarrow \\beta = (X^TX)^{-1}X^TY_i$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "(X^TX)^{-1} = (\n",
    "\\left\\lceil\n",
    "\\begin{matrix}\n",
    "1 & -2 & 0 & 3 \\\\\n",
    "2 & -2 & 1 & 2 \\\\\n",
    "\\end{matrix}\n",
    "\\right\\rceil\n",
    "\\left\\lceil\n",
    "\\begin{matrix}\n",
    "1 & 2 \\\\\n",
    "-2 & -2 \\\\\n",
    "0 & 1 \\\\\n",
    "3 & 1 \\\\\n",
    "\\end{matrix}\n",
    "\\right\\rceil)^{-1}\n",
    "= \n",
    "\\left\\lceil\n",
    "\\begin{matrix}\n",
    "14 & 9 \\\\\n",
    "9 & 10 \\\\\n",
    "\\end{matrix}\n",
    "\\right\\rceil^{-1}\n",
    "=\n",
    "\\left\\lceil\n",
    "\\begin{matrix}\n",
    "\\frac{10}{59} & \\frac{-9}{59} \\\\\n",
    "\\frac{-9}{59} & \\frac{14}{59} \\\\\n",
    "\\end{matrix}\n",
    "\\right\\rceil\n",
    "$$\n",
    "### Therefore, $ (X^TX)^{-1}X^TY_i = (X^TX)^{-1}(X^TX)\\beta$  is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\left\\lceil\n",
    "\\begin{matrix}\n",
    "\\frac{10}{59} & \\frac{-9}{59} \\\\\n",
    "\\frac{-9}{59} & \\frac{14}{59} \\\\\n",
    "\\end{matrix}\n",
    "\\right\\rceil\n",
    "\\left\\lceil\n",
    "\\begin{matrix}\n",
    "7 \\\\\n",
    "3 \\\\\n",
    "\\end{matrix}\n",
    "\\right\\rceil\n",
    "=\n",
    "\\left\\lceil\n",
    "\\begin{matrix}\n",
    "\\frac{10}{59} & \\frac{-9}{59} \\\\\n",
    "\\frac{-9}{59} & \\frac{14}{59} \\\\\n",
    "\\end{matrix}\n",
    "\\right\\rceil\n",
    "\\left\\lceil\n",
    "\\begin{matrix}\n",
    "14 & 9  \\\\\n",
    "9 & 10  \\\\\n",
    "\\end{matrix}\n",
    "\\right\\rceil\n",
    "\\hat{\\beta}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\left\\lceil\n",
    "\\begin{matrix}\n",
    "\\frac{43}{59}  \\\\\n",
    "\\frac{-21}{59} \\\\\n",
    "\\end{matrix}\n",
    "\\right\\rceil\n",
    "=\n",
    "\\left\\lceil\n",
    "\\begin{matrix}\n",
    "1 & 0  \\\\\n",
    "0 & 1 \\\\\n",
    "\\end{matrix}\n",
    "\\right\\rceil\n",
    "\\hat{\\beta}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\left\\lceil\n",
    "\\begin{matrix}\n",
    "\\hat{\\beta_1}\\\\\n",
    "\\hat{\\beta_2} \\\\\n",
    "\\end{matrix}\n",
    "\\right\\rceil\n",
    "=\n",
    "\\left\\lceil\n",
    "\\begin{matrix}\n",
    "\\frac{43}{59}  \\\\\n",
    "\\frac{-21}{59} \\\\\n",
    "\\end{matrix}\n",
    "\\right\\rceil\n",
    "=\n",
    "\\left\\lceil\n",
    "\\begin{matrix}\n",
    "0.7288  \\\\\n",
    "-0.3559 \\\\\n",
    "\\end{matrix}\n",
    "\\right\\rceil\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{7.2}$\n",
    "\n",
    "Fit a multiple linear regression with intercept. Find the parameters $\\beta_0$, $\\beta_1$, $\\beta_2$. Write the answer\n",
    "in $\\textbf{fractional form}$ (Hint. You may need to manipulate $\\textbf{X}$ to find the intercept) [5pt]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exist intercpet means that \n",
    "\n",
    "$$ Y_i = \\beta_0 + x_{i1}\\beta_1+ x_{i2}\\beta_2 +x_{i3}\\beta_3 + x_{i4}\\beta_4 + \\epsilon  $$\n",
    "Because, it has 4 of $x$ variables, we can transform into matrix form\n",
    "$$ Y_i = \\hat{\\beta_0} + X\\hat{\\beta} + \\epsilon$$\n",
    "\n",
    "Also, we can transform one more time, that \n",
    "$$ Y_i = \\beta_01^{4Ã—1} + x_{i1}\\beta_1+ x_{i2}\\beta_2 +x_{i3}\\beta_3 + x_{i4}\\beta_4 + \\epsilon  $$\n",
    "$$\\Rightarrow  Y_i =  + X^{\\prime}\\hat{\\beta} + \\epsilon$$\n",
    "$$ \\textbf{Where } X^{\\prime} = \n",
    "\\left\\lceil\n",
    "\\begin{matrix}\n",
    "1\\\\\n",
    "\\end{matrix}\n",
    "|x\n",
    "\\right\\rceil\n",
    "=\n",
    "\\left\\lceil\n",
    "\\begin{matrix}\n",
    "1 & 1 & 2 \\\\\n",
    "1 & -2 & -2 \\\\\n",
    "1 & 0 & 1 \\\\\n",
    "1 & 3 & 1 \\\\\n",
    "\\end{matrix}\n",
    "\\right\\rceil\n",
    "$$\n",
    "\n",
    "\n",
    "$ \\textbf{In order to find }\\hat{\\beta,} \\text{ we use Transform and inverse}$\n",
    "\n",
    "1) , we neglect the error term,\n",
    "\n",
    "$ Y_i = X^{\\prime}\\hat{\\beta} + \\epsilon \\approx Y_i = X^{\\prime}\\hat{\\beta} $\n",
    "\n",
    "$$\n",
    "\\\\\n",
    "\\left\\lceil\n",
    "\\begin{matrix}\n",
    "0 \\\\\n",
    "1 \\\\\n",
    "2 \\\\\n",
    "3 \\\\\n",
    "\\end{matrix}\n",
    "\\right\\rceil\n",
    "=\n",
    "\\left\\lceil\n",
    "\\begin{matrix}\n",
    "1 & 1 & 2 \\\\\n",
    "1 & -2 & -2 \\\\\n",
    "1 & 0 & 1 \\\\\n",
    "1 & 3 & 1 \\\\\n",
    "\\end{matrix}\n",
    "\\right\\rceil\n",
    "\\hat{\\beta}\n",
    "$$\n",
    "\n",
    "2) , we multiply $X^{\\prime^T}$ at both side, in order to make square matrix\n",
    "\n",
    "$ X^{\\prime^T}Y_i = (X^{\\prime^T}X^{\\prime})\\beta$\n",
    "\n",
    "$$\n",
    "\\\\\n",
    "\\left\\lceil\n",
    "\\begin{matrix}\n",
    "1 & 1 & 1 & 1 \\\\\n",
    "1 & -2 & 0 & 3 \\\\\n",
    "2 & -2 & 1 & 1 \\\\\n",
    "\\end{matrix}\n",
    "\\right\\rceil\n",
    "\\left\\lceil\n",
    "\\begin{matrix}\n",
    "0 \\\\\n",
    "1 \\\\\n",
    "2 \\\\\n",
    "3 \\\\\n",
    "\\end{matrix}\n",
    "\\right\\rceil\n",
    "=\n",
    "\\left\\lceil\n",
    "\\begin{matrix}\n",
    "1 & 1 & 1 & 1 \\\\\n",
    "1 & -2 & 0 & 3 \\\\\n",
    "2 & -2 & 1 & 1 \\\\\n",
    "\\end{matrix}\n",
    "\\right\\rceil\n",
    "\\left\\lceil\n",
    "\\begin{matrix}\n",
    "1 & 1 & 2 \\\\\n",
    "1 & -2 & -2 \\\\\n",
    "1 & 0 & 1 \\\\\n",
    "1 & 3 & 1 \\\\\n",
    "\\end{matrix}\n",
    "\\right\\rceil\n",
    "\\hat{\\beta}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\\\ \\Rightarrow\n",
    "\\left\\lceil\n",
    "\\begin{matrix}\n",
    "6 \\\\\n",
    "7 \\\\\n",
    "3 \\\\\n",
    "\\end{matrix}\n",
    "\\right\\rceil\n",
    "=\n",
    "\\left\\lceil\n",
    "\\begin{matrix}\n",
    "4 & 2 & 2  \\\\\n",
    "2 & 14 & 9  \\\\\n",
    "2 & 9 & 10  \\\\\n",
    "\\end{matrix}\n",
    "\\right\\rceil\n",
    "\\hat{\\beta}\n",
    "$$\n",
    "\n",
    "$\\textbf{1.}$ Then, multiply inverse matrix $(X^{\\prime^T}X^{\\prime})^{-1} $, in order to find $\\beta$\n",
    "\n",
    "$\\rightarrow (X^{\\prime^T}X^{\\prime})^{-1}X^{\\prime^T}Y_i = (X^{\\prime^T}X^{\\prime})^{-1}(X^{\\prime^T}X^{\\prime})\\beta$ $\\Rightarrow \\beta = (X^{\\prime^T}X^{\\prime})^{-1}X^{\\prime^T}Y_i$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "(X^{\\prime^T}X^{\\prime})^{-1} = (\n",
    "\\left\\lceil\n",
    "\\begin{matrix}\n",
    "1 & 1 & 1 & 1 \\\\\n",
    "1 & -2 & 0 & 3 \\\\\n",
    "2 & -2 & 1 & 1 \\\\\n",
    "\\end{matrix}\n",
    "\\right\\rceil\n",
    "\\left\\lceil\n",
    "\\begin{matrix}\n",
    "1 & 1 & 2 \\\\\n",
    "1 & -2 & -2 \\\\\n",
    "1 & 0 & 1 \\\\\n",
    "1 & 3 & 1 \\\\\n",
    "\\end{matrix}\n",
    "\\right\\rceil^{-1}\n",
    "= \n",
    "\\left\\lceil\n",
    "\\begin{matrix}\n",
    "4 & 2 & 2 \\\\\n",
    "2 & 14 & 9 \\\\\n",
    "2 & 9 & 10 \\\\\n",
    "\\end{matrix}\n",
    "\\right\\rceil^{-1}\n",
    "=\n",
    "\\left\\lceil\n",
    "\\begin{matrix}\n",
    "\\frac{59}{212} & \\frac{-1}{106} & \\frac{-5}{106} \\\\\n",
    "\\frac{-1}{106} & \\frac{9}{53} & \\frac{-8}{53}  \\\\\n",
    "\\frac{-5}{106} & \\frac{-8}{53} & \\frac{13}{53}  \\\\\n",
    "\\end{matrix}\n",
    "\\right\\rceil\n",
    "$$\n",
    "### Therefore, $ (X^{\\prime^T}X^{\\prime})^{-1}X^{\\prime^T}Y_i = (X^{\\prime^T}X^{\\prime})^{-1}(X^{\\prime^T}X^{\\prime})\\beta$  is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\left\\lceil\n",
    "\\begin{matrix}\n",
    "\\frac{59}{212} & \\frac{-1}{106} & \\frac{-5}{106} \\\\\n",
    "\\frac{-1}{106} & \\frac{9}{53} & \\frac{-8}{53}  \\\\\n",
    "\\frac{-5}{106} & \\frac{-8}{53} & \\frac{13}{53}  \\\\\n",
    "\\end{matrix}\n",
    "\\right\\rceil\n",
    "\\left\\lceil\n",
    "\\begin{matrix}\n",
    "6 \\\\\n",
    "7 \\\\\n",
    "3 \\\\\n",
    "\\end{matrix}\n",
    "\\right\\rceil\n",
    "=\n",
    "\\left\\lceil\n",
    "\\begin{matrix}\n",
    "\\frac{59}{212} & \\frac{-1}{106} & \\frac{-5}{106} \\\\\n",
    "\\frac{-1}{106} & \\frac{9}{53} & \\frac{-8}{53}  \\\\\n",
    "\\frac{-5}{106} & \\frac{-8}{53} & \\frac{13}{53}  \\\\\n",
    "\\end{matrix}\n",
    "\\right\\rceil\n",
    "\\left\\lceil\n",
    "\\begin{matrix}\n",
    "4 & 2 & 2  \\\\\n",
    "2 & 14 & 9  \\\\\n",
    "2 & 9 & 10 \\\\\n",
    "\\end{matrix}\n",
    "\\right\\rceil\n",
    "\\hat{\\beta}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\left\\lceil\n",
    "\\begin{matrix}\n",
    "\\frac{155}{106}  \\\\\n",
    "\\frac{36}{53} \\\\\n",
    "\\frac{-32}{53} \\\\\n",
    "\\end{matrix}\n",
    "\\right\\rceil\n",
    "=\n",
    "\\left\\lceil\n",
    "\\begin{matrix}\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 1 & 0  \\\\\n",
    "0 & 0 & 1  \\\\\n",
    "\\end{matrix}\n",
    "\\right\\rceil\n",
    "\\hat{\\beta}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\left\\lceil\n",
    "\\begin{matrix}\n",
    "\\hat{\\beta_0}\\\\\n",
    "\\hat{\\beta_1} \\\\\n",
    "\\hat{\\beta_2} \\\\\n",
    "\\end{matrix}\n",
    "\\right\\rceil\n",
    "=\n",
    "\\left\\lceil\n",
    "\\begin{matrix}\n",
    "\\frac{155}{106}  \\\\\n",
    "\\frac{36}{53} \\\\\n",
    "\\frac{-32}{53} \\\\\n",
    "\\end{matrix}\n",
    "\\right\\rceil\n",
    "=\n",
    "\\left\\lceil\n",
    "\\begin{matrix}\n",
    "1.462  \\\\\n",
    "0.6972 \\\\\n",
    "-0.6038 \\\\\n",
    "\\end{matrix}\n",
    "\\right\\rceil\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{7.3}$\n",
    "\n",
    "Compare your results using $\\textit{Python}$. Use the skeleton code given below. Paste your codes and\n",
    "screen capture the results of intercepts and coe\u001d",
    "cients for both question A and B. [5pt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries . Only the following will be needed\n",
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "\n",
    "X = np.array ([[1 ,2] ,[ -2 , -2] ,[0 ,1] ,[3 ,1]])\n",
    "y = np.array ([0 ,1 ,2 ,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model intercept :  0.0\n",
      "\n",
      "model coef :  [ 0.72881356 -0.3559322 ]\n"
     ]
    }
   ],
   "source": [
    "model = linear_model.LinearRegression(fit_intercept = False)\n",
    "model.fit(X, y)\n",
    "\n",
    "print(\"model intercept : \" , model.intercept_)\n",
    "print()\n",
    "print(\"model coef : \", model.coef_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model intercept :  1.4622641509433962\n",
      "\n",
      "model coef :  [ 0.67924528 -0.60377358]\n"
     ]
    }
   ],
   "source": [
    "model = linear_model.LinearRegression(fit_intercept = True)\n",
    "model.fit(X, y)\n",
    "\n",
    "print(\"model intercept : \" , model.intercept_)\n",
    "print()\n",
    "print(\"model coef : \", model.coef_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
